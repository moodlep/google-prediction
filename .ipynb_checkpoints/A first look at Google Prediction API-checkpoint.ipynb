{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "http://cloudacademy.com/blog/google-prediction-api/\n",
    "http://cloudacademy.com/blog/google-vision-api-image-analysis/\n",
    "https://www.youtube.com/watch?v=O3mfuc-syTI\n",
    "\n",
    "Video from IO:\n",
    "https://cloud.google.com/prediction/docs\n",
    "\n",
    "# Steps to first test\n",
    "\n",
    "Referencing the first link from cloudacademy - very good reference. \n",
    "\n",
    "* Downloaded data from https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones\n",
    "to git_repos folder. \n",
    "\n",
    "* installed google api client for python: \n",
    "pip install --upgrade google-api-python-client\n",
    "\n",
    "It looks like the free version of prediction api allows up to 5MB of trained data per day so I need to split the files: \n",
    "* Used split to split the file (60MB) into files of size 1MB. \n",
    "split -b 1m X_train.txt\n",
    "\n",
    "This creates lots of 1MB files labelled xaa, xab, etc. \n",
    "Next count the number of lines in the file: wc -l x_train1.txt   Result is 116 lines. \n",
    "\n",
    "Ditch the new file and just split off the first 116 lines off both training files using head. Just need a smaller sample file for now. Should ideally shuffle the file first. \n",
    "\n",
    "head -116 X_train.txt > x_train1.txt\n",
    "head -116 y_train.txt > y_train1.txt\n",
    "\n",
    "So training files are done. Now get the test files - maybe use 1/4 of this record count. \n",
    "head -30 X_test.txt > x_test1.txt\n",
    "head -30 y_test.txt > y_test1.txt\n",
    "\n",
    "Copy this into our repo. \n",
    "\n",
    "Ran script from cloudacademy which basically concats the data into one file and adds the y data as the first column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import izip\n",
    "import re\n",
    "\n",
    "output_file = 'dataset.csv'\n",
    "input_files = {\n",
    "    'human_activities/x_train1.txt': 'human_activities/y_train1.txt', \n",
    "    'human_activities/x_test1.txt': 'human_activities/y_test1.txt'\n",
    "}\n",
    "\n",
    "def getOutputLines(filenames):\n",
    "    for X,y in filenames.iteritems():\n",
    "        with open(X) as Xf, open(y) as yf:\n",
    "            for Xline, yline in izip(Xf, yf):\n",
    "                Xline = re.sub(' +', ' ', Xline).strip() #remove multiple white spaces and strip\n",
    "                yield ','.join([yline.strip()] + Xline.split(' ')) + \"\\n\" #concat in csv format\n",
    "                \n",
    "\n",
    "\n",
    "with open(output_file, 'w+') as f:\n",
    "    for newline in getOutputLines(input_files):\n",
    "        f.writelines(newline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next followed the instructions to create a project and a service account on the cloudacademy blog. \n",
    "All worked fine. json file downloaded. \n",
    "Check git repo for the details of the files. \n",
    "* new project created\n",
    "* prediction api activated for project\n",
    "* new service account created for project\n",
    "* json credential file for the service account downloaded locally \n",
    "* had to amend part of the code \n",
    "    get_api referenced \"credentials = client.SignedJwtAssertionCredentials(email, key, scope=scope)\" \n",
    "    had to replace this with: \n",
    "        from oauth2client.service_account import ServiceAccountCredentials\n",
    "        ....\n",
    "        credentials = ServiceAccountCredentials.from_json_keyfile_name('Human Interact-672e0199b3c7.json', scope)\n",
    "* thereafter it worked but I did run it from the command line. \n",
    "* note the prediction is based on a single record in a record.csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import httplib2, argparse, os, sys, json\n",
    "from oauth2client import tools, file, client\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "#Project and model configuration\n",
    "project_id = 'human-interact'\n",
    "#model_id is something we decide. \n",
    "model_id = 'HAR-model'\n",
    "\n",
    "#activity labels\n",
    "labels = {\n",
    "\t'1': 'walking', '2': 'walking upstairs', \n",
    "\t'3': 'walking downstairs', '4': 'sitting', \n",
    "\t'5': 'standing', '6': 'laying'\n",
    "}\n",
    "\n",
    "\n",
    "def main():\n",
    "\t\"\"\" Simple logic: train and make prediction\n",
    "        This caters for the async nature of the call/process. \n",
    "        The very first time the train_model will be called async\n",
    "        The model will be created and when this code is called again\n",
    "        the make_prediction will have access to and use the existing model\n",
    "    \"\"\"\n",
    "\ttry:\n",
    "\t\tmake_prediction()\n",
    "\texcept HttpError as e: \n",
    "\t\tif e.resp.status == 404: #model does not exist\n",
    "\t\t\tprint(\"Model does not exist yet.\")\n",
    "\t\t\ttrain_model()\n",
    "\t\t\tmake_prediction()\n",
    "\t\telse: #real error\n",
    "\t\t\tprint(e)\n",
    "\n",
    "def make_prediction():\n",
    "\t\"\"\" Use trained model to generate a new prediction \"\"\"\n",
    "\n",
    "\tapi = get_prediction_api()\n",
    "\t\n",
    "\tprint(\"Fetching model.\")\n",
    "\n",
    "\tmodel = api.trainedmodels().get(project=project_id, id=model_id).execute()\n",
    "\n",
    "\tif model.get('trainingStatus') != 'DONE':\n",
    "\t\tprint(\"Model is (still) training. \\nPlease wait and run me again!\") #no polling\n",
    "\t\texit()\n",
    "\n",
    "\tprint(\"Model is ready.\")\n",
    "\t\n",
    "\t\"\"\"\n",
    "\t#Optionally analyze model stats (big json!)\n",
    "\tanalysis = api.trainedmodels().analyze(project=project_id, id=model_id).execute()\n",
    "\tprint(analysis)\n",
    "\texit()\n",
    "\t\"\"\"\n",
    "\n",
    "\t#read new record from local file\n",
    "\twith open('record.csv') as f:\n",
    "\t\trecord = f.readline().split(',') #csv\n",
    "\n",
    "\t#obtain new prediction\n",
    "\tprediction = api.trainedmodels().predict(project=project_id, id=model_id, body={\n",
    "\t\t'input': {\n",
    "\t\t\t'csvInstance': record\n",
    "\t\t},\n",
    "\t}).execute()\n",
    "\n",
    "\t#retrieve classified label and reliability measures for each class\n",
    "\tlabel = prediction.get('outputLabel')\n",
    "\tstats = prediction.get('outputMulti')\n",
    "\n",
    "\t#show results\n",
    "\tprint(\"You are currently %s (class %s).\" % (labels[label], label) ) \n",
    "\tprint(stats)\n",
    "            \n",
    "            \n",
    "def train_model():\n",
    "\t\"\"\" Create new classification model \"\"\"\n",
    "\n",
    "\tapi = get_prediction_api()\n",
    "\n",
    "\tprint(\"Creating new Model.\")\n",
    "\n",
    "\tapi.trainedmodels().insert(project=project_id, body={\n",
    "\t\t'id': model_id,\n",
    "\t\t'storageDataLocation': 'human-interact-dataset/dataset.csv',\n",
    "\t\t'modelType': 'CLASSIFICATION'\n",
    "\t}).execute()\n",
    "\n",
    "def get_prediction_api(service_account=True):\n",
    "    scope = [\n",
    "        'https://www.googleapis.com/auth/prediction',\n",
    "        'https://www.googleapis.com/auth/devstorage.read_only'\n",
    "    ]\n",
    "    return get_api('prediction', scope, service_account)\n",
    "\n",
    "def get_api(api, scope, service_account=True):\n",
    "\t\"\"\" Build API client based on oAuth2 authentication \"\"\"\n",
    "\tSTORAGE = file.Storage('oAuth2.json') #local storage of oAuth tokens\n",
    "\tcredentials = STORAGE.get()\n",
    "\tif credentials is None or credentials.invalid: #check if new oAuth flow is needed\n",
    "\t\tif service_account: #server 2 server flow\n",
    "\t\t\twith open('Human Interact-672e0199b3c7.json') as f:\n",
    "\t\t\t\taccount = json.loads(f.read())\n",
    "\t\t\t\temail = account['client_email']\n",
    "\t\t\t\tkey = account['private_key']\n",
    "\t\t\tcredentials = ServiceAccountCredentials.from_json_keyfile_name('Human Interact-672e0199b3c7.json', scope)\n",
    "\t\t\tSTORAGE.put(credentials)\n",
    "\t\telse: #normal oAuth2 flow\n",
    "\t\t\tCLIENT_SECRETS = os.path.join(os.path.dirname(__file__), 'client_secrets.json')\n",
    "\t\t\tFLOW = client.flow_from_clientsecrets(CLIENT_SECRETS, scope=scope)\n",
    "\t\t\tPARSER = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter, parents=[tools.argparser])\n",
    "\t\t\tFLAGS = PARSER.parse_args(sys.argv[1:])\n",
    "\t\t\tcredentials = tools.run_flow(FLOW, STORAGE, FLAGS)\n",
    "\t\t\n",
    "\t#wrap http with credentials\n",
    "\thttp = credentials.authorize(httplib2.Http())\n",
    "\treturn discovery.build(api, \"v1.6\", http=http)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and comments\n",
    "\n",
    "* Simple example using data uploaded to a cloud storage bucket which is easily accessed by the prediction api. \n",
    "Api runs async and local code needs to poll the api to get the response. \n",
    "\n",
    "* So the model HAR-model would have been created after the training and any prediction requests would be based off the model thereafter and should be faster. In order to update the model you need to run the update api. \n",
    "The list of apis in python are here: \n",
    "https://developers.google.com/resources/api-libraries/documentation/prediction/v1.6/python/latest/prediction_v1.6.trainedmodels.html\n",
    "\n",
    "* analyze - special mention of this api is made in the article as it returns a confusion matrix which is quite useful. \n",
    "\n",
    "* When submitting the training data the y values are prepended to the X matrix as column 1 and the test and train data are consolidated. \n",
    "\n",
    "* google provides some performance enhancement suggestions including the minimum amount of features and data that should be provided. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
